{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook-Implemantacion-y-entrenamiento-red-neuronal.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diiYo7cR_ffD"
      },
      "source": [
        "# Entrenamiento de una neurona simple\n",
        "\n",
        "Vamos a comenzar por implementar una neurona simple definiendola con los parametros que hemos visto.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBuXLgF7tJ9x"
      },
      "source": [
        "### Neurona simple\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Djracm-Hu14j"
      },
      "source": [
        "import numpy as np "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdWm4GEg_yVE"
      },
      "source": [
        "Definimos la función de activación, por ejemplo una sigmoide"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OFqeNV3_yFE"
      },
      "source": [
        "def sigmoid(x):\n",
        "  # Our activation function: f(x) = 1 / (1 + e^(-x))\n",
        "  return ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycKAUjR1_1PZ"
      },
      "source": [
        "Definimos una clase que sea nuestra neurona"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_xK-uEw_rQQ"
      },
      "source": [
        "class Neuron:\n",
        "  def __init__(self, weights, bias):\n",
        "    self.weights = ##\n",
        "    self.bias = ##\n",
        "\n",
        "  def feedforward(self, inputs):\n",
        "    # Weight inputs, add bias, then use the activation function\n",
        "    total = ##\n",
        "    return sigmoid(total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXfvcEpl_97E"
      },
      "source": [
        "Probamos lo que hemos hecho hasta ahora"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVfFWIZU_eiI",
        "outputId": "2c4391fe-7eed-479f-b09f-86b32f67fc47"
      },
      "source": [
        "weights = np.array([0, 1]) \n",
        "bias = 4\n",
        "n = Neuron(weights, bias)\n",
        "\n",
        "x = np.array([2, 3])      \n",
        "print(n.feedforward(x)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9990889488055994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4MGXMf9Cvwd"
      },
      "source": [
        "## Entrenamiento de una red neuronal\n",
        "\n",
        "Vamos a proceder a entrenar ahora una red neuronal.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nM7z7_TRZsF"
      },
      "source": [
        "### Los datos \n",
        "Vamos a usar unos datos dummies para ver como funciona la red."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbPGBHxYOZEk"
      },
      "source": [
        "from sklearn.datasets import make_moons\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = make_moons(n_samples = 1000, noise=0.2, random_state=1993)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1993)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd6DM0g9FBH-"
      },
      "source": [
        "### Inicialización de las capas de la red neuronal\n",
        "\n",
        "Lo primero que vamos a hacer, como en cualquier entrenamiento de redes neuronales, es llevar a cabo una inicialización que nos sirva de punto de partida para los pesos de las neuronas y sus bias.\n",
        "\n",
        "Para este cometido, vamos a emplear un vector de diccionarios con la información de cada una de las capas de la red "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnntiVxjkR8N",
        "outputId": "d993d5e1-9666-4eb8-b76a-5c4c567d5457"
      },
      "source": [
        "X.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCDeCsgbFckc"
      },
      "source": [
        "nn_architecture = [\n",
        "    {\"input_dim\": 2, \"output_dim\": 25, \"activation\": \"relu\"},\n",
        "    {\"input_dim\": 25, \"output_dim\": 50, \"activation\": \"relu\"},\n",
        "    {\"input_dim\": 50, \"output_dim\": 50, \"activation\": \"relu\"},\n",
        "    {\"input_dim\": 50, \"output_dim\": 25, \"activation\": \"relu\"},\n",
        "    {\"input_dim\": 25, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y82B5HLNGrny"
      },
      "source": [
        "Aqui podemos observar como en la definición de los parámetros se debe respetar el rango de dimension de la entrada de una capa con la salida de la anterior.\n",
        "\n",
        "¿Cuántas entradas aceptará nuestra red?\n",
        "\n",
        "Procedemos ahora a utilizar esta información para inicializar las capas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w63u7aBqGPnK"
      },
      "source": [
        "def inicializar_capas(nn_architecture, seed = 1993):\n",
        "    np.random.seed(seed)\n",
        "    number_of_layers = ###\n",
        "    parametros = ###\n",
        "\n",
        "    for idx, layer in enumerate(nn_architecture):\n",
        "        layer_idx = idx + 1\n",
        "        layer_input_size = layer[\"input_dim\"]\n",
        "        layer_output_size = layer[\"output_dim\"]\n",
        "        \n",
        "        parametros['W' + str(layer_idx)] = ###\n",
        "        parametros['b' + str(layer_idx)] = ###\n",
        "        \n",
        "    return parametros"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVU5u3ICJGAW"
      },
      "source": [
        "¿Por qué esta inicialización? Por la ruptura de la simetría"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P43Nnb-PJHRn"
      },
      "source": [
        "### Definición de la función de activación\n",
        "Vamos a proceder ahora a definir distintas funciones de activación que hemos visto durante la clase pero ahora en formato código "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBEHr1xDJFvd"
      },
      "source": [
        "def sigmoid(Z):\n",
        "  sigmoid_func = ###\n",
        "  return sigmoid_func\n",
        "\n",
        "def relu(Z):\n",
        "  relu_func = ###\n",
        "  return relu_func"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpxOIxlBJtw1"
      },
      "source": [
        "Para poder hacer en los pasos de Backpropagation o actualización de parametros, calcularemos las funciones que nos devuelvan estos valores.\n",
        "\n",
        "Estos valores osn la derivada de la función de activación multiplicada por el incremento de cambio\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLneiCB7IzSo"
      },
      "source": [
        "def sigmoid_backward(dA, Z):\n",
        "    sig = sigmoid(Z)\n",
        "    return dA * sig * (1 - sig)\n",
        "\n",
        "def relu_backward(dA, Z):\n",
        "    dZ = np.array(dA, copy = True)\n",
        "    dZ[Z <= 0] = 0;\n",
        "    return dZ;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spbz6FJ3LB72"
      },
      "source": [
        "### Cálculo del Forward propagation\n",
        "Este paso sería el cálculo de la salida de nuestra red neuronal, igual que hemos visto en clase pero esta vez aplicado a multiples capas.\n",
        "\n",
        "Lo primero que vamos a hacer es definir el comportamiento interno de cada una de las capas que tendremos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDjpX7RgJEBq"
      },
      "source": [
        "def single_layer_forward_propagation(A_prev,\n",
        "                                     W_act, \n",
        "                                     b_act, \n",
        "                                     activation=\"relu\"):\n",
        "  \n",
        "\n",
        "  '''' Definimos una funcion que haga el calculo de cada una de las \n",
        "  neuronas que compone a las neuronas de una capa\n",
        "  '''\n",
        "  Z_act = np.dot(W_act, A_prev) + b_act\n",
        "  \n",
        "  if activation is \"relu\":\n",
        "      funcion_de_activacion = ###\n",
        "  elif activation is \"sigmoid\":\n",
        "      funcion_de_activacion = ###\n",
        "\n",
        "  A_act = ###\n",
        "  return A_act, Z_act"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-z2dOcmNFXz"
      },
      "source": [
        "Con una capa construida, ya podemos construir el total de la red utilizando la función que hemos creado previamente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3bveS8WNQZM"
      },
      "source": [
        "def full_forward_propagation(X, \n",
        "                             params_values, \n",
        "                             nn_architecture):\n",
        "    memory = {}\n",
        "    A_act = X\n",
        "    \n",
        "    for idx, layer in enumerate(nn_architecture):\n",
        "        layer_idx = ###\n",
        "        A_prev = ###\n",
        "        \n",
        "        activ_function_act = ###\n",
        "        W_act = ###\n",
        "        b_act = ###\n",
        "        A_act, Z_act = single_layer_forward_propagation(A_prev, \n",
        "                                                        W_act, \n",
        "                                                        b_act, \n",
        "                                                        activ_function_act)\n",
        "        \n",
        "        memory[\"A\" + str(idx)] = A_prev\n",
        "        memory[\"Z\" + str(layer_idx)] = Z_act\n",
        "       \n",
        "    return A_act, memory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr54IPUvRxuj"
      },
      "source": [
        "### Función de perdida\n",
        "Como hemos visto en clase, es necesario definir una función de perdida para poder llevar a cabo el descenso del graciente y que en cada iteración se evalue.\n",
        "\n",
        "Como en este caso estamos haciendo un problema de clasificación usaremos la CrossEntropy\n",
        "![picture](https://drive.google.com/uc?id=1AiroSwCaqDt9tOyaLd0iX8v1kt23UChC)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah8k83RvNqQ1"
      },
      "source": [
        "def get_cost_value(Y_pred, Y):\n",
        "    m = Y_pred.shape[1]\n",
        "    cost = -1 / m * (np.dot(Y, np.log(Y_pred).T) + np.dot(1 - Y, np.log(1 - Y_pred).T))\n",
        "    return np.squeeze(cost)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAuJIAobUulO"
      },
      "source": [
        "### Calculo del Backward Propagation\n",
        "\n",
        "Para actualizar los pesos y bias de las neuronas de nuestra red, hemos dicho que es necesario llevar a cabo un \"camino hacia atras\" que nos permita actualizar estos parametros desde las ultimas capas a las primeras mediante el uso de la derivada del gradiente parcial por cada una de las variables que componen a la red.\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1Z3idGhAIAk8PoDktTxg4IoZtEax9oCDM)\n",
        "\n",
        "Vamos a programar el paso en cada una de las capas para que sea más sencillo su entendimiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8YEdSZlUu8k"
      },
      "source": [
        "def single_layer_backward_propagation(dA_act,\n",
        "                                      W_act,\n",
        "                                      b_act,\n",
        "                                      Z_act,\n",
        "                                      A_prev,\n",
        "                                      f_act=\"relu\"):\n",
        "    m = A_prev.shape[1]\n",
        "    if f_act is \"relu\":\n",
        "        backward_activation_func = relu_backward\n",
        "    elif f_act is \"sigmoid\":\n",
        "        backward_activation_func = sigmoid_backward\n",
        "    \n",
        "    dZ_act = backward_activation_func(dA_act, Z_act)\n",
        "    dW_act = np.dot(dZ_act, A_prev.T) / m\n",
        "    db_act = np.sum(dZ_act, axis=1, keepdims=True) / m\n",
        "    dA_prev = np.dot(W_act.T, dZ_act)\n",
        "\n",
        "    return dA_prev, dW_act, db_act"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jod7XMUmWl22"
      },
      "source": [
        "Y ahora lo aplicamos para todas las capas "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uetiL32RWok4"
      },
      "source": [
        "def full_backward_propagation(Y_hat, \n",
        "                              Y, \n",
        "                              memory, \n",
        "                              params_values, \n",
        "                              nn_architecture):\n",
        "    grads_values = {}\n",
        "    m = Y.shape[0]\n",
        "    Y = Y.reshape(Y_hat.shape)\n",
        "   \n",
        "    dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));\n",
        "    \n",
        "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
        "        layer_idx_act = layer_idx_prev + 1\n",
        "        activ_function_act = layer[\"activation\"]\n",
        "        \n",
        "        dA_act = dA_prev\n",
        "        \n",
        "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
        "        Z_act = memory[\"Z\" + str(layer_idx_act)]\n",
        "        W_act = params_values[\"W\" + str(layer_idx_act)]\n",
        "        b_act = params_values[\"b\" + str(layer_idx_act)]\n",
        "        \n",
        "        dA_prev, dW_act, db_act = single_layer_backward_propagation(\n",
        "            dA_act, W_act, b_act, Z_act, A_prev, activ_function_act)\n",
        "        \n",
        "        grads_values[\"dW\" + str(layer_idx_act)] = dW_act\n",
        "        grads_values[\"db\" + str(layer_idx_act)] = db_act\n",
        "    \n",
        "    return grads_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fz94IfGZg1L"
      },
      "source": [
        "### Actualización de los parametros\n",
        "\n",
        "Una vez que tenmos los valores que se modifican los parametros de nuestra red los actualizamos teniendo en cuenta un coeficiente que ya conocemos, el learning rate.\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1FmeaacqxB9hnzplZ-3Nn7seZkMVzk8rH)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4B7V5ALZhWC"
      },
      "source": [
        "def actualizar_parametros(parametros, \n",
        "                          grads_values,\n",
        "                          nn_architecture,\n",
        "                          learning_rate):\n",
        "    for layer_idx, layer in enumerate(nn_architecture, 1):\n",
        "        parametros[\"W\" + str(layer_idx)] -= ###   \n",
        "        parametros[\"b\" + str(layer_idx)] -= ###\n",
        "\n",
        "    return parametros"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAaZGx_yaepP"
      },
      "source": [
        "### Unificamos todos los pasos para el entrenamiento\n",
        "\n",
        "Ponemos ahora todo en común para obtener el entrenamiento de nuestra red."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkrPCXpyaosU"
      },
      "source": [
        "def train(X, Y, \n",
        "          nn_architecture, \n",
        "          epochs, \n",
        "          learning_rate):\n",
        "  \n",
        "    parametros = inicializar_capas(nn_architecture, 2)\n",
        "    cost_history = []    \n",
        "\n",
        "    for i in range(epochs):\n",
        "        Y_hat, cashe = full_forward_propagation(X, parametros, nn_architecture)\n",
        "        cost = get_cost_value(Y_hat, Y)\n",
        "        cost_history.append(cost)    \n",
        "\n",
        "        grads_values = full_backward_propagation(Y_hat, Y, cashe, parametros, nn_architecture)\n",
        "        parametros = actualizar_parametros(parametros, grads_values, nn_architecture, learning_rate)\n",
        "      \n",
        "    return parametros, cost_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAE_lC1RrzpF"
      },
      "source": [
        "### Entrenamos y comprobamos rendimiento\n",
        "\n",
        "Obtenemos los parametros obtenidos de nuestro entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3ZCKO91jjDF"
      },
      "source": [
        "train_param = train(np.transpose(X_train), \n",
        "                      np.transpose(y_train.reshape((y_train.shape[0], 1))),\n",
        "                      nn_architecture, 10000, 0.01)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jo6KbXBZr962"
      },
      "source": [
        "Calculamos con forward propagation la salida de nuestro modelo para los test_values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OQgod1EpVBU"
      },
      "source": [
        "Y_test_hat, _ = full_forward_propagation(np.transpose(X_test), train_param, nn_architecture)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar8CANhPsGT2"
      },
      "source": [
        "Obtenemos la accuracy del modelo. Para ello nos servimos de esta función."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PxL2nzysLK0"
      },
      "source": [
        "def convert_prob_into_class(probs):\n",
        "    probs_ = np.copy(probs)\n",
        "    probs_[probs_ > 0.5] = 1\n",
        "    probs_[probs_ <= 0.5] = 0\n",
        "    return probs_\n",
        "\n",
        "def get_accuracy_value(Y_hat, Y):\n",
        "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
        "    return (Y_hat_ == Y).all(axis=0).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAje6WCEpo-P",
        "outputId": "0b2633bd-7cd4-4cde-d535-a0b0c0971bed"
      },
      "source": [
        "acc_test = get_accuracy_value(Y_test_hat, np.transpose(y_test.reshape((y_test.shape[0], 1))))\n",
        "print(\"Test set accuracy: {:.2f}\".format(acc_test))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set accuracy: 0.96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcSPXES-p1q-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}